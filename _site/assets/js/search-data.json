{"0": {
    "doc": "GAN stability(to be continued)",
    "title": "Stablized GAN training",
    "content": "The original GAN was known for unstable during the training: . | when the fake and real sample distribution do not overlap in the early phase of training(an intrinsic requirement by the Kullback Leibler divergence). | . \\[KL(P \\lVert Q) = \\sum_{x=1}^{N}P(x) log{\\frac{P(x)}{Q(x)}}\\] . | when one of the adversarial networks overpowers the other, neither one of the two will learn informative features over the training samples; . | the intrisinc design of the objective function contains unwanted properties that hinders the convergence; . | . Vanishing gradient . From the previous equation, one could assume generated sample has distribution with various mean(e.g. $\\mu$ = 0, $\\mu$=5, $\\mu=30$), while real sample of a normal distribution($\\mathcal{N}(1,0)$). The KL divergence for those generated sample distribution of $\\mu=35$ leads to saturating value, which cause the discriminator to rate 0 for $G(z), z\\in Q$. Thus the gradient of generator with regards to GAN loss function is: . Mode collapse . During the training, the generator always look for one type of output that is the most plausible for the discriminator, and the discriminator always reject the rest. Thus only monotomous output are generated by the generator even fed various inputs. \\[-\\nabla_{\\theta_{g}}log(1-D(G(z))) \\rightarrow 0\\] 1. Wasserstein GAN . The Wasserstein GAN(WGAN) is first known for effectively balance the GAN training. Where weight clipping and wasserstein metrics are used. The later describes the minimum cost of converting one distribution $q$ to another $p$, mathematically defined as the greatest lower bound(infimum) of a transport plan, this could be formulated as: \\(W(p,q) = inf_{\\gamma \\in \\Pi(\\mathbb{P_{r}, \\mathbb{P_{g}}})} \\mathbb{E}_{(x,y) \\sim \\gamma} [\\lVert x-y \\rVert]\\) where $\\Pi$ contains all the possible transport plan $\\gamma$. The above formula is intractable but could be further simplified to the follow, using Kantorovich-Rubinstein duality: . \\[W(\\mathbb{P_{r}}, \\mathbb{P_{g}}) = sup_{\\lVert f\\rVert_{L} \\leq1} \\; \\mathbb{E}_{x\\sim \\mathbb{P}_{r}}[f(x)] - \\mathbb{E}_{x\\sim \\mathbb{P}_{g}}[f(x)]\\] Where $f(\\cdot)$ is a Lipschitz-1 function, meaning: $\\lvert f(x_{1}) - f(x_{2})\\rvert \\leq \\lvert x_{1} - x_{2}\\rvert $, and such function $f(\\cdot)$ is parameterized by the critics(discriminator without the last non-linear layer). Or say, the well-trained critics is the function $f(\\cdot)$. As per the Lipischtz-1 constrain, the utilization of weight clipping explicitly clips the maximum magnetude of the critics to a constant range $c$. Different from the gradient of the original discriminator, . \\[\\nabla_{\\theta_{d}}\\frac{1}{N}\\sum_{i=1}^{N}[log(D(x^{(i)})) + log(1-D(G(x^{(i)})))]\\] it becomes: . \\[\\nabla_{\\omega}\\frac{1}{N}\\sum_{i=1}^{N}[f_{\\omega}(x^{(i)}) - f_{\\omega}(G(z^{(i)}))]\\] Drawbacks . Despite its early success in updating the generator at discriminator’s optimal, and stable training dynamics, the choise of weight clipping hyper-parameter is still tricky. Especially without batch normalization, the gradient could still vanish or explode on decreasing or increasing the magnitude of $c$. Secondly, the explicit constrain on weights also restricts the capacity of the model from learning complicated data or function. 2. WGAN-GP . A differentiable function is Lipschitz-1 continous if and only if its gradient norm is 1 everywhere. On top of WGAN work, the WGAN-GP instead of explicitly constrain Lipschitz-1 continuity via weight-clipping, it interpolates between real and generated data distribution to ensure gradient norm of 1. As is proven in the appendix A of the paper: \\(\\mathbb{P}_{(x,y)\\sim \\pi}[\\nabla f^{*}(x_{t}) = \\frac{y-x_{t}}{\\lVert y-x_{t}\\rVert}] = 1\\) Where $\\pi(x=y)=0$ and $x_{t} = tx_{g} + (1-t)x_{r}\\text{, with } 0\\leq t\\leq 1$. The WGAN loss improves to: . \\[W_{GP}(\\mathbb{P}_{r}, \\mathbb{P}_{g}) = \\mathbb{E}_{x\\sim \\mathbb{P}_{r}}[f(x)] - \\mathbb{E}_{x\\sim \\mathbb{P}_{g}}[f(x)] + \\lambda \\mathbb{E}_{x_{t}\\sim \\mathbb{P}_{x_{t}}}[(\\lVert \\nabla_{x_{t}}(D(x_{t}))\\rVert_{2} - 1)^{2}]\\] Differently, WGAN-GP does not use batch normalization to avoid creating correlation among data inside mini-batch. Pros . The WGAN-GP variant offers better convergence and more constistent performance after convergence, compared to DCGAN. Eventhough it requires more computation complexity. ",
    "url": "/GAN%20stabilty.html#stablized-gan-training",
    
    "relUrl": "/GAN%20stabilty.html#stablized-gan-training"
  },"1": {
    "doc": "GAN stability(to be continued)",
    "title": "The overlooked roles of the discriminator",
    "content": "The discriminator is commonly regarded as one player of the competing game, while it actually acts a measurement of the divergence. ",
    "url": "/GAN%20stabilty.html#the-overlooked-roles-of-the-discriminator",
    
    "relUrl": "/GAN%20stabilty.html#the-overlooked-roles-of-the-discriminator"
  },"2": {
    "doc": "GAN stability(to be continued)",
    "title": "GAN stability(to be continued)",
    "content": " ",
    "url": "/GAN%20stabilty.html",
    
    "relUrl": "/GAN%20stabilty.html"
  },"3": {
    "doc": "Projects",
    "title": "Projects",
    "content": "This is a personal website on sharing knowledge. Github . ",
    "url": "/Projects.html",
    
    "relUrl": "/Projects.html"
  },"4": {
    "doc": "Resume",
    "title": "Resume",
    "content": "Here is the link to my resume: Resume . ",
    "url": "/resume/",
    
    "relUrl": "/resume/"
  },"5": {
    "doc": "Super resolution on MRI",
    "title": "Super resolution using GAN",
    "content": "This project is mainly deploying GAN for generating high fedility MRI images . The result of SR could be extend for consequent applications, e.g. data augmentation for segmentation models, and these result were shown in OHBM2022 and MIDL2022. ",
    "url": "/Super-resolution.html#super-resolution-using-gan",
    
    "relUrl": "/Super-resolution.html#super-resolution-using-gan"
  },"6": {
    "doc": "Super resolution on MRI",
    "title": "Super resolution on MRI",
    "content": " ",
    "url": "/Super-resolution.html",
    
    "relUrl": "/Super-resolution.html"
  },"7": {
    "doc": "About me",
    "title": "About me",
    "content": "I'm currently a 4th year Ph.D. student at Universität Tübingen and Max Planck Institute for Biological Cybernetics. I was educated in major of Mechanical Engineering in Hongkong Polytechnic University. My current research falls in generative models using deep learning methods(e.g. GAN, score-match model, flow-based methods.) and its application on Medical Image data, such as MRI(~9.4Tesla). Apart from applications, I'm also studying the following topics: . | the convergence properties and training dynamics of GAN | the application of optimal transport on deep learning tasks | the alignment of machine intelligence with functional connectivity in human brain | . ",
    "url": "/",
    
    "relUrl": "/"
  },"8": {
    "doc": "About me",
    "title": "Publications",
    "content": "DISGAN: Wavelet-informede discriminator guides GAN to MRI images super-resolution with noise cleaning submitted to ICCV2023 Workshop on Computer Vision for Automated Medical Diagnosis Qi Wang, Lucas Mahler, Julius Steiglechner, Florian Birk, Klaus Scheffler, Gabriele Lohmann . A Three-player GAN for Super-Resolution in Magnetic Resonance Imaging submitted to MICCAI2023 Workshop on Machine Learning for Clinical Neuroimaging Qi Wang, Lucas Mahler, Julius Steiglechner, Florian Birk, Klaus Scheffler, Gabriele Lohmann . METAFormer: A Multi-Atlas Enhanced Transformer Architecture for Autism Spectrum Disorder Classification Using Resting-State fMRI submitted to MICCAI2023 Workshop on Machine Learning for Clinical Neuroimaging Lucas Mahler, Qi Wang, Julius Steiglechner, Florian Birk, Samuel Heczko, Klaus Scheffler, Gabriele Lohmann . Pretraining is All You Need: A Multi-Atlas Enhanced Transformer Framework for Autism Spectrum Disorder Classification preprint Lucas Mahler, Qi Wang, Julius Steiglechner, Florian Birk, Samuel Heczko, Klaus Scheffler, Gabriele Lohmann . A Three-Player GAN for Super-Resolution in Magnetic Resonance Imaging preprint Qi Wang, Lucas Mahler, Julius Steiglechner, Florian Birk, Klaus Scheffler, Gabriele Lohmann . Super-Resolution for Ultra High-Field MR Images Medical Imaging with Deep Learning(MIDL 2022) Qi Wang, Julius Steiglechner, Tobias Lindig, Benjamin Bender, Klaus Scheffler, Gabriele Lohmann . FLEXseg: Next Generation Brain MRI Segmentation at 9.4 T preprint Julius Steiglechner, Qi Wang, Dana Ramadan, Lucas Mahler, Klaus Scheffler, Benjamin Bender, Tobias Lindig, Gabriele Lohmann . Focal fMRI signal enhancement with implantable inductively coupled detectors NeuroImage 2022 Yi Chen#, Qi Wang#, Sangcheon Choi, Hang Zeng, Kengo Takahashi, Chunqi Qian, Xin Yu#: Joint first author . Inductively coupled detectors for optogenetic-driven focal and multiregional fMRI signal enhancement ISMRM 2021, summa cum laude award Yi Chen#, Qi Wang#, Sangcheon Choi, Hang Zeng, Kengo Takahashi, Chunqi Qian, Xin Yu #: Joint first author . Real-Time fMRI Brain Mapping in Animals Journal of Visualized Experiments, 2020 Sangcheon Choi, Kengo Takahashi, Yuanyuan Jiang, Sascha K&ouml;hler, Hang Zeng, Qi Wang, Yan Ma, Xin Yu . ",
    "url": "/#publications",
    
    "relUrl": "/#publications"
  }
}
