{"0": {
    "doc": "Blogs",
    "title": "Blogs",
    "content": "This is a personal website on sharing knowledge. Github . ",
    "url": "/Blogs.html",
    
    "relUrl": "/Blogs.html"
  },"1": {
    "doc": "GAN stability(to be continued)",
    "title": "Stablized GAN training",
    "content": "The original GAN was known for unstable during the training: . | when the fake and real sample distribution do not overlap in the early phase of training(an intrinsic requirement by the Kullback Leibler divergence). | . \\[KL(P \\lVert Q) = \\sum_{x=1}^{N}P(x) log{\\frac{P(x)}{Q(x)}}\\] . | when one of the adversarial networks overpowers the other, neither one of the two will learn informative features over the training samples; . | the intrisinc design of the objective function contains unwanted properties that hinders the convergence; . | . Vanishing gradient . From the previous equation, one could assume generated sample has distribution with various mean(e.g. $\\mu$ = 0, $\\mu$=5, $\\mu=30$), while real sample of a normal distribution($\\mathcal{N}(1,0)$). The KL divergence for those generated sample distribution of $\\mu=35$ leads to saturating value, which cause the discriminator to rate 0 for $G(z), z\\in Q$. Thus the gradient of generator with regards to GAN loss function is: . Mode collapse . During the training, the generator always look for one type of output that is the most plausible for the discriminator, and the discriminator always reject the rest. Thus only monotomous output are generated by the generator even fed various inputs. \\[-\\nabla_{\\theta_{g}}log(1-D(G(z))) \\rightarrow 0\\] 1. Wasserstein GAN . The Wasserstein GAN(WGAN) is first known for effectively balance the GAN training. Where weight clipping and wasserstein metrics are used. The later describes the minimum cost of converting one distribution $q$ to another $p$, mathematically defined as the greatest lower bound(infimum) of a transport plan, this could be formulated as: \\(W(p,q) = inf_{\\gamma \\in \\Pi(\\mathbb{P_{r}, \\mathbb{P_{g}}})} \\mathbb{E}_{(x,y) \\sim \\gamma} [\\lVert x-y \\rVert]\\) where $\\Pi$ contains all the possible transport plan $\\gamma$. The above formula is intractable but could be further simplified to the follow, using Kantorovich-Rubinstein duality: . \\[W(\\mathbb{P_{r}}, \\mathbb{P_{g}}) = sup_{\\lVert f\\rVert_{L} \\leq1} \\; \\mathbb{E}_{x\\sim \\mathbb{P}_{r}}[f(x)] - \\mathbb{E}_{x\\sim \\mathbb{P}_{g}}[f(x)]\\] Where $f(\\cdot)$ is a Lipschitz-1 function, meaning: $\\lvert f(x_{1}) - f(x_{2})\\rvert \\leq \\lvert x_{1} - x_{2}\\rvert $, and such function $f(\\cdot)$ is parameterized by the critics(discriminator without the last non-linear layer). Or say, the well-trained critics is the function $f(\\cdot)$. As per the Lipischtz-1 constrain, the utilization of weight clipping explicitly clips the maximum magnetude of the critics to a constant range $c$. Different from the gradient of the original discriminator, . \\[\\nabla_{\\theta_{d}}\\frac{1}{N}\\sum_{i=1}^{N}[log(D(x^{(i)})) + log(1-D(G(x^{(i)})))]\\] it becomes: . \\[\\nabla_{\\omega}\\frac{1}{N}\\sum_{i=1}^{N}[f_{\\omega}(x^{(i)}) - f_{\\omega}(G(z^{(i)}))]\\] Drawbacks . Despite its early success in updating the generator at discriminator’s optimal, and stable training dynamics, the choise of weight clipping hyper-parameter is still tricky. Especially without batch normalization, the gradient could still vanish or explode on decreasing or increasing the magnitude of $c$. Secondly, the explicit constrain on weights also restricts the capacity of the model from learning complicated data or function. 2. WGAN-GP . A differentiable function is Lipschitz-1 continous if and only if its gradient norm is 1 everywhere. On top of WGAN work, the WGAN-GP instead of explicitly constrain Lipschitz-1 continuity via weight-clipping, it interpolates between real and generated data distribution to ensure gradient norm of 1. As is proven in the appendix A of the paper: \\(\\mathbb{P}_{(x,y)\\sim \\pi}[\\nabla f^{*}(x_{t}) = \\frac{y-x_{t}}{\\lVert y-x_{t}\\rVert}] = 1\\) Where $\\pi(x=y)=0$ and $x_{t} = tx_{g} + (1-t)x_{r}\\text{, with } 0\\leq t\\leq 1$. The WGAN loss improves to: . \\[W_{GP}(\\mathbb{P}_{r}, \\mathbb{P}_{g}) = \\mathbb{E}_{x\\sim \\mathbb{P}_{r}}[f(x)] - \\mathbb{E}_{x\\sim \\mathbb{P}_{g}}[f(x)] + \\lambda \\mathbb{E}_{x_{t}\\sim \\mathbb{P}_{x_{t}}}[(\\lVert \\nabla_{x_{t}}(D(x_{t}))\\rVert_{2} - 1)^{2}]\\] Differently, WGAN-GP does not use batch normalization to avoid creating correlation among data inside mini-batch. Pros . The WGAN-GP variant offers better convergence and more constistent performance after convergence, compared to DCGAN. Eventhough it requires more computation complexity. ",
    "url": "/GAN%20stabilty.html#stablized-gan-training",
    
    "relUrl": "/GAN%20stabilty.html#stablized-gan-training"
  },"2": {
    "doc": "GAN stability(to be continued)",
    "title": "The overlooked roles of the discriminator",
    "content": "The discriminator is commonly regarded as one player of the competing game, while it actually acts a measurement of the divergence. ",
    "url": "/GAN%20stabilty.html#the-overlooked-roles-of-the-discriminator",
    
    "relUrl": "/GAN%20stabilty.html#the-overlooked-roles-of-the-discriminator"
  },"3": {
    "doc": "GAN stability(to be continued)",
    "title": "GAN stability(to be continued)",
    "content": " ",
    "url": "/GAN%20stabilty.html",
    
    "relUrl": "/GAN%20stabilty.html"
  },"4": {
    "doc": "Gradient flow",
    "title": "Gradient flow",
    "content": "Prelimiaries . Flux: The amount of mass ($q$) pass through a unit area per unit time ($kg/m^2/s$). Or informally, it’s a product of density of the fluid times a n-D vector of velocity on a topological surface: . \\[\\vec{j} = \\rho \\vec{u}\\] $\\text{where }\\vec{u}\\text{ is the velocity field.}$ . Continuity equation: the transport process of mass of conserved material. \\[\\frac{\\partial{\\rho}}{\\partial{t}} + \\nabla \\vec{j} = \\sigma ,\\] \\[\\text{where density, divergence, flux and generation of mass is noted as } \\rho, \\nabla, \\vec{j}, \\text{ and } \\sigma.\\] Probability perspective . As an analogy to mass conservation in physics world, probability mass is also considered conservative when being mapped from one to another. Evolution of pdf. Here, the overall probability mass of a moving distribution ($\\mathbb{Q}$) is mapped to the target distribution ($\\mathbb{P}$). The probability density, $q_{t}(x)$, is analogous to the density of mass which evolves overtime (${q_{t}}\\text{, }t\\in \\mathbb{R}^{+}$), and finally mapped to anothre probability density ($p$). Metrics space. Unlike the cartisian coordinates in physics space, the “flux” in the measurement context refers to: . \\[\\begin{split}-\\text{flux}&amp;=-\\nabla f(x_{t})\\\\&amp;=\\text{div}(q_{t}\\nabla_{metircs}\\mathcal{F}(q_{t}))\\\\&amp;=\\text{div}(q_{t}\\nabla_{x}\\frac{\\partial{\\mathcal{F(q_{t})}}}{\\partial{q_{t}}}) \\end{split}\\] $\\text{where the last equation can be seen as the gradient of the measurement }\\mathcal{F(\\cdot)}\\text{ on Wasserstein space}$ . Deep learning persepctive (gradient descent) . ",
    "url": "/Gradient%20flow.html",
    
    "relUrl": "/Gradient%20flow.html"
  },"5": {
    "doc": "Resume",
    "title": "Resume",
    "content": "Here is the link to my CV . ",
    "url": "/Resume.html",
    
    "relUrl": "/Resume.html"
  },"6": {
    "doc": "Super resolution on MRI",
    "title": "Super resolution using GAN",
    "content": "This project is mainly about deploying GAN for generating high fedility MRI images . The result of SR could be extend for consequent applications, e.g. data augmentation for segmentation models, and these result were shown in OHBM2022 and MIDL2022. In my opinion, SR task should be distinguished from other geenrative tasks (e.g. style transfer, cross-modality generation etc.), thus I personally prefer to redeem a SR task as restoring HR image from a LR conterpart with minimum pereceptual loss (i.e. indistinguishable for human perception). While many models fails to this point, SwinIR or DDPM treat SR as a way of matching the probability distribution of HR and SR images, which overlooks the local details that are mostly approximated via the intrinsic inductive bias of the convolutional layers. ",
    "url": "/Super-resolution.html#super-resolution-using-gan",
    
    "relUrl": "/Super-resolution.html#super-resolution-using-gan"
  },"7": {
    "doc": "Super resolution on MRI",
    "title": "Super resolution on MRI",
    "content": " ",
    "url": "/Super-resolution.html",
    
    "relUrl": "/Super-resolution.html"
  },"8": {
    "doc": "About me",
    "title": "About me",
    "content": ". I'm currently a 4th year Ph.D. student at Universität Tübingen and Max Planck Institute for Biological Cybernetics. Prior to that, I recieved my master in major of Mechanical Engineering in Hongkong Polytechnic University. My current research falls in generative models using deep learning methods(e.g. GAN, score-match model, flow-based methods.) and its application on Medical Image data, such as MRI(~9.4Tesla). Apart from applications, I'm also working on the following topics: . | the convergence properties and training dynamics of GAN | the application of optimal transport on deep learning tasks | the alignment of machine intelligence with functional connectivity in human brain | . ",
    "url": "/",
    
    "relUrl": "/"
  },"9": {
    "doc": "About me",
    "title": "Publications",
    "content": "Preprints . Improving the reliability of fMRI-based predictions of intelligence via semi-blind machine learning Preprint, 2023 Gabriele Lohmann, Samuel Heczko, Lucas Mahler, Qi Wang, Julius Steiglechner, Vinod J. Kumar, Michelle Roost, Jürgen Jost, Klaus Scheffler . METAFormer: A Multi-Atlas Enhanced Transformer Architecture for Autism Spectrum Disorder Classification Using Resting-State fMRI Preprint, 2023 Lucas Mahler, Qi Wang, Julius Steiglechner, Florian Birk, Samuel Heczko, Klaus Scheffler, Gabriele Lohmann . A Three-Player GAN for Super-Resolution in Magnetic Resonance Imaging Preprint, 2022 Qi Wang, Lucas Mahler, Julius Steiglechner, Florian Birk, Klaus Scheffler, Gabriele Lohmann . FLEXseg: Next Generation Brain MRI Segmentation at 9.4 T Preprint, 2022 Julius Steiglechner, Qi Wang, Dana Ramadan, Lucas Mahler, Klaus Scheffler, Benjamin Bender, Tobias Lindig, Gabriele Lohmann . Conference papers . DISGAN: Wavelet-informede discriminator guides GAN to MRI images super-resolution with noise cleaning ICCV2023 Workshop on Computer Vision for Automated Medical Diagnosis Qi Wang, Lucas Mahler, Julius Steiglechner, Florian Birk, Klaus Scheffler, Gabriele Lohmann . A Three-player GAN for Super-Resolution in Magnetic Resonance Imaging MICCAI2023 Workshop on Machine Learning for Clinical Neuroimaging Qi Wang, Lucas Mahler, Julius Steiglechner, Florian Birk, Klaus Scheffler, Gabriele Lohmann . Pretraining is All You Need: A Multi-Atlas Enhanced Transformer Framework for Autism Spectrum Disorder Classification MICCAI2023 Workshop on Machine Learning for Clinical Neuroimaging Lucas Mahler, Qi Wang, Julius Steiglechner, Florian Birk, Samuel Heczko, Klaus Scheffler, Gabriele Lohmann . Super-Resolution for Ultra High-Field MR Images Medical Imaging with Deep Learning (MIDL 2022, Zürich) Qi Wang, Julius Steiglechner, Tobias Lindig, Benjamin Bender, Klaus Scheffler, Gabriele Lohmann . Synthetic 9T-like structural MRI using a Generative Neural Network Neurowissenschaftliche Nachwuchskonferenz (NeNa 2021, Tübingen) Qi Wang, Juliu Steiglechner, Gabriele Lohmann . Focal fMRI signal enhancement with implantable inductively coupled detectors NeuroImage 2022 Yi Chen#, Qi Wang#, Sangcheon Choi, Hang Zeng, Kengo Takahashi, Chunqi Qian, Xin Yu#: Joint first author . Inductively coupled detectors for optogenetic-driven focal and multiregional fMRI signal enhancement ISMRM 2021, summa cum laude award Yi Chen#, Qi Wang#, Sangcheon Choi, Hang Zeng, Kengo Takahashi, Chunqi Qian, Xin Yu #: Joint first author . Real-Time fMRI Brain Mapping in Animals Journal of Visualized Experiments, 2020 Sangcheon Choi, Kengo Takahashi, Yuanyuan Jiang, Sascha K&ouml;hler, Hang Zeng, Qi Wang, Yan Ma, Xin Yu . Talks . A Three-Player GAN for Super-Resolution in Magnetic Resonance Imaging Oral presentation on MICCAI 2023 MLCN workshop, Paris, France . Deep learning for MRI super resolution and its applications Max Planck Institute for Intelligent System, Tübingen, Germany . Super Resolution Improves Cortical Segmentation Accuracy in Ultra-high Resolution MRI International Conference on Human Brain Mapping (OHBM) 2022, Glasgow, UK . Sythetic 9T-like structural MRI using Generative Neural Network 22nd Conference of Junior Neuroscientists (NeNa 2021), Tübingen, Germany . ",
    "url": "/#publications",
    
    "relUrl": "/#publications"
  }
}
